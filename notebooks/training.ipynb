{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Quaternion PyTorch - Training a QNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from htorch import quaternion, layers, utils"
   ]
  },
  {
   "source": [
    "### Step 1 - Loading the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We provide a `collate_fn` to convert any standard image dataset to a quaternion format (by using the RGB values as imaginary components and the greyscale version as real component)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n",
      "170499072it [06:02, 470083.88it/s]                               \n",
      "Extracting data\\cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "# Standard loading for the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "data = CIFAR10(root='data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data using a custom collate_fn function to convert to quaternion-valued images\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=8, shuffle=True, \\\n",
    "    collate_fn=utils.convert_data_for_quaternion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([8, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(loader))\n",
    "print(xb.shape) # We now have 4 input channels as needed"
   ]
  },
  {
   "source": [
    "### Step 2 - Building the QNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We use a simple QNN with three convolutional blocks with split-ReLU activations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    layers.QConv2d(1, 20, kernel_size=10, bias=True), # We only have 1 channel in terms of quaternions\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2), # Max-pool is okay because it acts on the channels\n",
    "    layers.QConv2d(20, 20, kernel_size=10, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.Flatten(),\n",
    "    layers.QLinear(20, 10),\n",
    "    layers.QuaternionToReal(10), # Take the absolute value before the softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([8, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Test the model is working correctly\n",
    "model(xb).shape"
   ]
  },
  {
   "source": [
    "### Step 3 - Training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "At this point, everything is classical PyTorch:\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,  2000] loss: 1.733\n",
      "[1,  4000] loss: 1.468\n",
      "[1,  6000] loss: 1.393\n",
      "[2,  2000] loss: 1.240\n",
      "[2,  4000] loss: 1.218\n",
      "[2,  6000] loss: 1.189\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "source": [
    "### Converting an existing nn.Module\n",
    "\n",
    "Using the new [torch.fx](https://pytorch.org/docs/stable/fx.html) functionals, we can also convert an existing PyTorch's `nn.Module` into a quaternion-valued one, provided all shapes are divisible by 4:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is similar to the previous one, but all dimensions are multiplied by 4\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(4, 80, kernel_size=10, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.Conv2d(80, 80, kernel_size=10, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(80, 40),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([8, 40])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Convert to a QNN and run on the previous set of images\n",
    "utils.convert_to_quaternion(model)(xb).shape"
   ]
  }
 ]
}